{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Accuracy, precision, and recall\n\n.. figure:: /_static/imgs/accuracy.png\n\n## Accuracy\n\n### Formula\n\n\\begin{align}\\dfrac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total number of samples}}\\end{align}\n\n\n### Interpretation\n\nThe proportion of correctly classified samples. It is an overall indicator\nof the model's performance (irrespective of sample sizes and class/category (im)balance).\n\n.. caution::\n\n    Accuracy ignores possible biases introduced by unbalanced sample sizes. I.e. it\n    ignores the \"two ways of being wrong\" problem (see below).\n\n### Example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. jupyter-execute::\n    :hide-code:\n\n    from pathlib import Path\n    import requests\n    import matplotlib.pyplot as plt\n    import matplotlib.font_manager as fm\n    import matplotlib.patches as patches\n    import seaborn as sns\n    sns.set_style(\"darkgrid\")\n\n    # Use the raw.githubusercontent.com domain for direct access to the raw file\n    github_url = 'https://raw.githubusercontent.com/google/fonts/master/ofl/librebaskerville/LibreBaskerville-Regular.ttf'\n\n    response = requests.get(github_url)\n    response.raise_for_status()  # Raise an exception for bad responses\n\n    temp_file = Path(\"LibreBaskerville-Regular.ttf\")\n    with temp_file.open(\"wb\") as f:\n       f.write(response.content)\n\n    font_prop = fm.FontProperties(fname=temp_file)\n    font_dict = {'fontproperties': font_prop, \"size\": 14}\n\n    fig, ax = plt.subplots()\n    # Set x-axis range\n    ax.set_xlim((1,9))\n    ax.set_xticks([])\n    # Set y-axis range\n    ax.set_ylim((1,9))\n    ax.set_yticks([])\n    ax.set_ylabel(r\"Model $\\Theta(\\hat{y})$\", **font_dict)\n\n    # Draw lines to split quadrants\n    ax.plot([5,5],[1,9], linewidth=2, color=\"#e5e5e2\")\n    ax.plot([1,9],[5,5], linewidth=2, color=\"#e5e5e2\")\n    ax.set_title('Reality', **font_dict)\n\n    tomato = \"#FE5431E4\"\n    blue = \"#0090FF\"\n    width, height = 4, 4\n\n    top_left_square = patches.Rectangle(\n        (1, 5), width, height, linewidth=1, edgecolor='none', facecolor=blue\n    )\n    top_right_square = patches.Rectangle(\n        (5, 5), width, height, linewidth=1, edgecolor='none', facecolor=tomato\n    )\n    bottom_left_square = patches.Rectangle(\n        (1, 1), width, height, linewidth=1, edgecolor='none', facecolor=tomato\n    )\n    bottom_right_square = patches.Rectangle(\n        (5, 1), width, height, linewidth=1, edgecolor='none', facecolor=blue\n    )\n\n    ax.add_patch(top_left_square)\n    ax.add_patch(top_right_square)\n    ax.add_patch(bottom_left_square)\n    ax.add_patch(bottom_right_square)\n\n    # Add labels\n    ax.text(2, 8, \"True Positive\", **font_dict)\n    ax.text(6, 8, \"False Positive\", **font_dict)\n    ax.text(2, 4, \"False Negative\", **font_dict)\n    ax.text(6, 4, \"True Negative\", **font_dict)\n\n    # Add numbers\n    text_dict = {\"fontsize\": 18, \"weight\": \"bold\", \"family\": \"sans-serif\"}\n    ax.text(2.5, 7, 60, **text_dict) # True Positive\n    ax.text(6.5, 7, 25, **text_dict) # False Positive\n    ax.text(6.5, 3, 15, **text_dict) # True Negative\n    ax.text(2.5, 3, 0, **text_dict) # False Negative\n    fig.suptitle(\"Cat or dog: Actual cat photos vs model predictions (N=100)\", **font_dict)\n    plt.show()\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nN = 100\nsamples = np.array([\"cat\"] * 60 + [\"dog\"] * 40)\n# Model always predicts cat\npredictions = np.array([\"cat\"] * 85 + [\"dog\"] * 15)\n\nprint(f\"Number of cats in sample: {np.sum(samples == 'cat')}\")\nprint(f\"Number of dogs in sample: {np.sum(samples == 'dog')}\")\nprint(f\"Number of cats that model predicts are in sample: {np.sum(predictions == 'cat')}\")\n\ntotal = len(samples)\nTP = np.sum((samples == \"cat\") & (predictions == \"cat\")) # True Positives\nTN = np.sum((samples == \"dog\") & (predictions == \"dog\")) # True Negatives\nFP = np.sum((samples == \"dog\") & (predictions == \"cat\")) # False Positives\nFN = np.sum((samples == \"cat\") & (predictions == \"dog\")) # False Negatives\naccuracy = (TP + TN) / total\nprint(f\"Accuracy = ({TP} + {TN}) / {total} = {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For every photo of a cat, the model correctly categorizes the photo as cat. But notice\nthat the model also incorreclty categorizes 15 dog photos as cat. Do you think that\nthe accuracy score sufficiently captures the model's performance?\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precision\n\n### Formula\n\n\\begin{align}\\dfrac{\\text{True Positives}}{\\text{Total number of \"yes\" predictions}}\\end{align}\n\n### Interpretation\n\nReveals when the model has a bias towards saying \"yes\". Precision includes a penalty\nfor misclassifying negative samples as positive. This is useful when the cost of\nmisclassifying a negative sample as positive is high. For example, in fraud detection.\n\n\n### Example\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "precision = TP / (TP + FP)\nprint(f\"Accuracy = ({TP} + {TN}) / {total} = {accuracy}\")\nprint(f\"Precision = {TP} / ({TP} + {FP})\", f\"{precision:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The precision score is 0.7. This means that the model is correct 70% of the time\nwhen it predicts \"cat\". This is lower than the accuracy of .75 because the model\nis being penalized for misclassifying dog photos as cat photos. Do you think that\nthis improves upon the accuracy score?\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recall (aka Sensitivity)\n\n### Formula\n\n\\begin{align}\\dfrac{\\text{True Positives}}{\\text{Total number of actual positive samples}}\\end{align}\n\n### interpretation\n\nReveals when the model has a bias for saying \"no\". It includes a penalty for false\nnegatives. This is useful when the cost of classifying a positive sample as\nnegative is high, for example, in cancer detection.\n\n### Example\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "recall = TP / (TP + FN)\nprint(f\"Recall = {TP} / ({TP} + {FN}) = {recall:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The recall score is 1.0 (100%!), which indicates that the model does NOT have a bias\ntowards saying \"no\". We can see this in the data, as the model doesn't misclassify any\ncat photo as dog photos.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## F1\n\nThe F1 score basically accuracy, precision, and recall into a single metric.\n\n### Formula\n\n\\begin{align}\\dfrac{ \\text{True Positives} }{ \\text{True Positives} + \\dfrac{1}{2} (\\text{False Positives} + \\text{False Negatives}) }\\end{align}\n\nThe denominator is True Positives plus the average of the \"two ways of being wrong\".\n\n### Interpretation\n\nThe F1 provides a balance between precision and recall. It gives a general idea of\nwhether the model is biased (but doesn't tell you which way it is biased). An F1\nscore is high when the model makes few mistakes.\n\n### Example\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f1 = TP / (TP + (np.mean(FP + FN)))\nprint(f\"F1 = {TP} / ({TP} + (1/2)*({FP} + {FN})) = {f1:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\nAs a reminder, here are the scores we calculated:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Accuracy = {accuracy:.2f}\")\nprint(f\"Precision = {precision:.2f}\")\nprint(f\"Recall = {recall:.2f}\")\nprint(f\"F1 = {f1:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The F1 score is 0.71, do o you think this provides a good balance between precision,\nrecall, and accuracy?\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the relationship between, accuracy, precision, recall, and F1\n\nPeople often say that the F1 score is a \"harmonic mean\" of precision and recall.\nOne way to think of this, is that the F1 score will penalize the model for being\nbiased towards saying \"yes\" or \"no\".\n\n.. seealso::\n   [Stack Overflow: Harmonic Mean](https://stackoverflow.com/questions/26355942/why-is-the-f-measure-a-harmonic-mean-and-not-an-arithmetic-mean-of-the-precision)\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Let's run an experiment\nn = 50\nn_experiments = 10_000\n# initialize arrays to store the results\naccuracies = np.zeros(n_experiments)\nprecisions = np.zeros(n_experiments)\nrecalls = np.zeros(n_experiments)\nf1_scores = np.zeros(n_experiments)\n\n# run the experiment\nfor experiment in range(n_experiments):\n    # generate random data\n    tp = np.random.randint(1, n)\n    fn = n - tp\n    fp = np.random.randint(1, n)\n    tn = n - fp\n\n    # calculate metrics\n    accuracies[experiment] = (tp + tn) / (2*n)\n    precisions[experiment] = tp / (tp + fp)\n    recalls[experiment] = tp / (tp + fn)\n    f1_scores[experiment] = tp / (tp + (fp + fn) / 2)\n\n# plot the results\nsns.set_style(\"darkgrid\")\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), constrained_layout=True)\n\nfor this_ax, metric, title in zip(axes, [precisions, recalls], [\"Precision\", \"Recall\"]):\n    sc = this_ax.scatter(accuracies, f1_scores, c=metric, s=5)\n    this_ax.plot([0, 1], [.5, .5], color=\"black\", linestyle=\"--\", linewidth=.5)\n    this_ax.plot([.5, .5], [0, 1], color=\"black\", linestyle=\"--\", linewidth=.5)\n    this_ax.set_title(f\"F1-Accuracy vs {title}\")\n    this_ax.set_xlabel(\"Accuracy\")\n    this_ax.set_ylabel(\"F1-Score\")\n    # Add colorbar\n    cbar = fig.colorbar(sc, ax=this_ax)\n    cbar.set_label(title)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When Precision, Recall, and F1 are the same\n\nWhen would precision and recall be the same? Let's remind ourselves of the formulas:\n\n.. table::\n\n   +-----------------------------+-----------------------------+----------------------------------------------+\n   | Precision                   | Recall                      | F1 Score                                     |\n   +=============================+=============================+==============================================+\n   | $\\dfrac{TP}{TP + FP}$ | $\\dfrac{TP}{TP + FN}$ | $\\dfrac{TP}{TP + \\text{mean(FP + FN)}}$|\n   +-----------------------------+-----------------------------+----------------------------------------------+\n\nWhen will these fractions be equal? Well they only differ in the denominator, in\nthat precision cares about False Positives, recall cares about False Negatives, and\nF1 weighs them equally (it takes an average). So, if their denominators are equal,\nthen the fractions will be equal. Put differently, if the model makes the same\nnumber of False Positives as False Negatives, then it doesn't have a bias towards\nsaying \"yes\" or \"no\".\n\nTo add one more layer of complexity, if precision and recall are equal, then surely\nthe F1 score will be equal to them as well.. Because the average of two equal numbers\nis the same as the numbers themselves. This makes the F1 fraction equal to both the\nprecision and recall fractions. \n\nTo drive that point home, let's provide an example in code:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sympy import symbols, solve, simplify\n\n# Define variables for confusion matrix\ntp, tn, fp, fn = symbols('tp tn fp fn')\n\n# Basic metrics \naccuracy = (tp + tn) / (tp + tn + fp + fn)\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)\nf1 = 2 * (precision * recall) / (precision + recall)\n\n# Try to solve system of equations\n# We want the number of true positives to be equal to the number of false negatives\nsolution = solve([\n    fp - fn\n])\n\nprint(\"Solution space:\")\nprint(solution)\n\nprint(\"\\nMetrics in terms of remaining variables:\")\nprint(f\"Precision = {simplify(precision.subs(solution))}\")\nprint(f\"Recall = {simplify(recall.subs(solution))}\")\nprint(f\"F1 Score = {simplify(f1.subs(solution))}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}