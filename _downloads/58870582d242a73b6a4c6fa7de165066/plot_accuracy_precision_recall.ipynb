{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Accuracy, precision, and recall\n\n.. figure:: /_static/imgs/accuracy.png\n\n## Accuracy\n\n### Formula\n\n\\begin{align}\\dfrac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total number of samples}}\\end{align}\n\n\n### Interpretation\n\nThe proportion of correctly classified samples. It is an overall indicator\nof the model's performance (irrespective of sample sizes and class/category (im)balance).\n\n.. caution::\n\n    Accuracy ignores possible biases introduced by unbalanced sample sizes. I.e. it\n    ignores the \"two ways of being wrong\" problem (see below).\n\n## Precision\n\n### Formula\n\n\\begin{align}\\dfrac{\\text{True Positives}}{\\text{Total number of \"yes\" predictions}}\\end{align}\n\n### Interpretation\n\nReveals when the model has a bias towards saying \"yes\". Precision includes a penalty\nfor misclassifying negative samples as positive. This is useful when the cost of\nmisclassifying a negative sample as positive is high. For example, in fraud detection.\n\n## Recall (aka Sensitivity)\n\n### Formula\n\n\\begin{align}\\dfrac{\\text{True Positives}}{\\text{Total number of actual positive samples}}\\end{align}\n\n### interpretation\n\nReveals when the model has a bias for saying \"no\". It includes a penalty for false\nnegatives. This is useful when the cost of classifying a positive sample as\nnegative is high, for example, in cancer detection.\n\n\n## F1\n\nThe F1 score basically incorporates the 3 metrics above into a single metric.\n\n### Formula\n\n\\begin{align}\\dfrac{ \\text{True Positives} }{ \\text{True Positives} + \\dfrac{1}{2} (\\text{False Positives} + \\text{False Negatives}) }\\end{align}\n\nThe denominator is True Positives plus the average of the \"two ways of being wrong\".\n\n### Interpretation\n\nThe F1 provides a balance between precision and recall. It gives a general idea of\nwhether the model is biased (but doesn't tell you which way it is biased). An F1\nscore is high when the model makes few mistakes.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}