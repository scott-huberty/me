{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Compute Accuracy, Precision, Recall, and F1 on the Wine dataset\n\nWe'll use Scikit Learn to compute these metrics for us.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import (\n  accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n  )\nfrom sklearn.model_selection import train_test_split\n\n# for number-crunching\nimport numpy as np\n\n# for dataset management\nimport polars as pl\n\n# for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Wine dataset\nWe'll use the Wine dataset from the UCI Machine Learning Repository.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\ndf = pl.read_csv(url, separator=\";\", infer_schema_length=int(1e5))\ndf = df.rename(lambda col_name : col_name.replace(\" \", \"_\"))\n# Drop a few outliers\ndf = df.filter(df[\"total_sulfur_dioxide\"] < 200)\n\nz_scores = [\n    (pl.col(col) - pl.col(col).mean()) / pl.col(col).std()\n    for col in df.columns\n    if col != \"quality\"\n    ]\ndf = df.select([\n    pl.col(\"quality\"),\n    *z_scores\n])\n\n# create a new column for binarized (boolean) quality\ndf = df.with_columns(\n    pl.when(df[\"quality\"] > 5).then(1).otherwise(0).alias(\"good_quality\")\n)\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert to torch tensors\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_tensor = df.select(\n    [col for col in df.columns if col not in [\"quality\", \"good_quality\"]]\n).to_torch().float()\nlabels_tensor = df.select(\"good_quality\").to_torch().float()\nprint(f\"train_tensor shape: {train_tensor.shape}\", f\"labels_tensor shape: {labels_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split the data\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(train_tensor, labels_tensor, test_size=.1)\n# then convert them into PyTorch Datasets (note: already converted to tensors)\ntrain_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\ntest_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n\n# Finally, create the DataLoader objects\nn_samples = test_dataset.tensors[0].shape[0] \ntrain_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, drop_last=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=n_samples, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the model\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class WineNet(nn.Module):\n  def __init__(self):\n    super().__init__()\n\n    ### input layer\n    self.input = nn.Linear(11,16)\n    \n    ### hidden layers\n    self.fc1 = nn.Linear(16,32)\n    self.fc2 = nn.Linear(32,32)\n\n    ### output layer\n    self.output = nn.Linear(32,1)\n  \n  # forward pass\n  def forward(self,x):\n    x = F.relu( self.input(x) )\n    x = F.relu( self.fc1(x) )\n    x = F.relu( self.fc2(x) )\n    return self.output(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "wine_net = WineNet()\nnum_epochs = 500\n# loss function and optimizer\nlossfun = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(wine_net.parameters(), lr=.01)\n\n# initialize losses\nlosses   = torch.zeros(num_epochs)\ntrain_accuracies = []\ntest_accuracies  = []\n\n# loop over epochs\nfor epochi in range(num_epochs):\n    # loop over training data batches\n    batch_accuracies  = []\n    batch_losses = []\n    for x, y in train_loader:\n        # forward pass and loss\n        y_hat = wine_net(x)\n        loss = lossfun(y_hat , y)\n\n        # backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # loss from this batch\n        batch_losses.append(loss.item())\n\n        # compute training accuracy for this batch\n        accuracy = 100 * torch.mean(((y_hat > 0) == y).float()).item()\n        batch_accuracies.append(accuracy)\n        # end of batch loop...\n\n    # now that we've trained through the batches, get their average training accuracy\n    train_accuracies.append( np.mean(batch_accuracies) )\n\n    # and get average losses across the batches\n    losses[epochi] = np.mean(batch_losses)\n\n    # test accuracy\n    x, y = next(iter(test_loader)) # extract X, y from test dataloader\n    with torch.no_grad(): # deactivates autograd\n        y_hat = wine_net(x)\n        test_acc = 100 * torch.mean(((y_hat > 0) == y).float()).item()\n        test_accuracies.append(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the accuracy, precision, recall, and F1 score on the train and test sets\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_predictions = wine_net(train_loader.dataset.tensors[0])\ntest_predictions = wine_net(test_loader.dataset.tensors[0])\n\n# initialize a dictionary to store the metrics\ntrain_metrics = [0, 0, 0, 0]\ntest_metrics = [0, 0, 0, 0]\n\n# compute the metrics on the train set\ntrue_labels = train_loader.dataset.tensors[1]\ntrain_predictions = train_predictions > 0\ntrain_metrics[0] = accuracy_score(true_labels, train_predictions)\ntrain_metrics[1] = precision_score(true_labels, train_predictions)\ntrain_metrics[2] = recall_score(true_labels, train_predictions)\ntrain_metrics[3] = f1_score(true_labels, train_predictions)\n\n# compute the metrics on the test set\ntrue_labels = test_loader.dataset.tensors[1]\ntest_predictions = test_predictions > 0\ntest_metrics[0] = accuracy_score(true_labels, test_predictions)\ntest_metrics[1] = precision_score(true_labels, test_predictions)\ntest_metrics[2] = recall_score(true_labels, test_predictions)\ntest_metrics[3] = f1_score(true_labels, test_predictions)\nfor i, metric in enumerate(['Accuracy', 'Precision', 'Recall', 'F1-score']):\n    print(f'{metric} (train): {train_metrics[i]:.2f}')\n    print(f'{metric} (test): {test_metrics[i]:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the metrics\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sns.set_style(\"darkgrid\")\nfig, ax = plt.subplots()\n\nax.bar(np.arange(4) -.1, train_metrics, .5)\nax.bar(np.arange(4) +.1, test_metrics, .5)\nax.set_xticks([0, 1, 2, 3], ['Accuracy', 'Precision', 'Recall', 'F1-score'])\nax.set_ylim([.6,1])\nax.legend(['Train', 'Test'])\nax.set_title('Performance metrics')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show the confusion matrices\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confusion matrices\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "true_labels_train = train_loader.dataset.tensors[1]\ntrue_labels_test = test_loader.dataset.tensors[1]\ntrain_conf = confusion_matrix(true_labels_train, train_predictions>0)\ntest_conf  = confusion_matrix(true_labels_test, test_predictions>0)\n\nsns.set_style('white')\nfig, axes = plt.subplots(1, 2, figsize=(10,4))\n\n# Confusion Matrix (train)\naxes[0].imshow(train_conf, 'Blues', vmax=len(train_predictions)/2)\naxes[0].set_xticks([0,1])\naxes[0].set_yticks([0,1])\naxes[0].set_xticklabels(['bad','good'])\naxes[0].set_yticklabels(['bad','good'])\naxes[0].set_xlabel('Predicted quality')\naxes[0].set_ylabel('True quality')\naxes[0].set_title('TRAIN confusion matrix')\n\n# add text labels\ntext_kwargs = dict(ha='center', va='center')\naxes[0].text(0, 0, f'True negatives:\\n{train_conf[0, 0]}' , **text_kwargs)\naxes[0].text(0, 1, f'False negatives:\\n{train_conf[1, 0]}', **text_kwargs)\naxes[0].text(1, 1, f'True positives:\\n{train_conf[1, 1]}' , **text_kwargs)\naxes[0].text(1, 0, f'False positives:\\n{train_conf[0, 1]}', **text_kwargs)\n\n# Confusion Matrix (test)\naxes[1].imshow(test_conf, 'Blues', vmax=len(test_predictions)/2)\naxes[1].set_xticks([0,1])\naxes[1].set_yticks([0,1])\naxes[1].set_xticklabels(['bad','good'])\naxes[1].set_yticklabels(['bad','good'])\naxes[1].set_xlabel('Predicted quality')\naxes[1].set_ylabel('True quality')\naxes[1].set_title('TEST confusion matrix')\n\n# add text labels\naxes[1].text(0, 0, f'True negatives:\\n{test_conf[0,0]}', **text_kwargs)\naxes[1].text(0, 1, f'False negatives:\\n{test_conf[1,0]}', **text_kwargs)\naxes[1].text(1, 1, f'True positives:\\n{test_conf[1,1]}' , **text_kwargs)\naxes[1].text(1, 0, f'False positives:\\n{test_conf[0,1]}', **text_kwargs)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}