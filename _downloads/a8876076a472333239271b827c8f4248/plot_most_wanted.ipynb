{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Use ICA to isolate the instruments in a 100 Gecs song.\n\n<img src=\"https://i.kym-cdn.com/entries/icons/original/000/018/666/How_Do_You_Do_Fellow_Kids_meme_banner_image.jpg\">\n\n\nI really like ICA, and I use it all the time. But it's not the most intuitive algorithm,\nand it is relatively niche compared to supervised learning. While teaching my students\nabout it, I wanted to give them an example that was more intuitive than say, artifact\nremoval in EEG data (after all, ICA can be used for much more than just artifact\nremoval). So here's my attempt at that, and also my attempt to pretend like I\nknow what the kids are listening to these days.\n\nICA is a method that can be used to separate a complex signal (or image, or whatever\nelse) into its constituent parts.\n\nBut there's a catch. You need to have multiple \"observations\" of this signal.\nThe canonical example would be an orchestra performance recorded with multiple\nmicrophones (a few in each section). Each microphone is one \"observation\". Since\neach microphone picks up a blend of all the instruments, you can use ICA to separate\nthe individual instruments (so you'd have the violins isolated.. 0r the tuba.. etc.).\n\nAnd no, you probably can't use ICA to separate the vocals from a song you downloaded\nfrom the internet.\n\nSince a an orchestra performance might bore you (and because I don't have such\na recording handy), let's use a different example. Pretend you are in the studio with\n100 gecs. You set up 4 microphones in the room, each recording the drums, bass, an \neffects track (FX), and vocals. The individual microphones also pick\nup some of the other instruments, but that's okay. You can use ICA to separate the\ncomponents!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from functools import partial\nfrom pathlib import Path\n\nimport IPython.display as ipd\nimport numpy as np\nimport pooch\nfrom scipy.io import wavfile\nfrom sklearn.decomposition import FastICA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define some helper functions\n(You can skip this section if you're not interested in the details)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def load_audio(wav_path):\n    \"\"\"Load a wav file from disk.\"\"\"\n    return wavfile.read(wav_path)\n\n\ndef convert_to_mono(wav_array):\n    \"\"\"Convert stereo audio to mono by averaging the channels.\"\"\"\n    return np.mean(wav_array, axis=1)\n\n\ndef normalize_audio(wav_array):\n    \"\"\"Normalize the decibel range to -1 to 1.\"\"\"\n    return wav_array / np.max(np.abs(wav_array))\n\n\ndef process_audio(wav_path):\n    \"\"\"Load a wav file, convert stereo to mono, and normalize decibel range.\"\"\"\n    sfreq, wav_array = load_audio(wav_path)\n    if len(wav_array.shape) > 1:\n        wav_array = convert_to_mono(wav_array)\n    return sfreq, normalize_audio(wav_array)\n\n\ndef mix_stems(*wavs, mix_matrix):\n    \"\"\"Blend the individual stems together using a mixing matrix.\"\"\"\n    return np.dot(mix_matrix, np.array(wavs, dtype=float))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the mixed audio\nWe'll define a data fetcher to download the stems pack from the 10,000 gecs album.\nPlease note that this will download a 1.2 GB file to your machine. Please be patient!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Please be patient, this may take a while...\")\n# We will ignore the guitars stem because it is mostly silent\nwant_stems = [\"Drums.wav\", \"Bass.wav\", \"Vocals.wav\", \"FX.wav\"]\nmembers = [\n    f\"10,000 gecs Stems/The Most Wanted Person in the United States/{stem}\"\n    for stem in want_stems\n]\n\nunpack = pooch.Unzip(\n    extract_dir=\".\", # Relative to the path where the zip file is downloaded\n    members=members,\n)\n\nstem_fpaths = pooch.retrieve(\n    url=\"https://www.100gecs.com/uploads/10000gecsstems.zip\",\n    known_hash=\"sha256:65d2f8dc5cf61a6cd2ac722c2c3bef465b76ca50f5d0363425acdbc5b100e754\",\n    progressbar=True,\n    path=Path.home() / \"100gecs\",\n    processor=unpack,\n)\nstems_dir = Path(stem_fpaths[0]).parent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the stems\nWe'll load the stems and process them by converting stereo to mono and normalizing\nthe decibel range.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sfreq, drums = process_audio(stems_dir / \"Drums.wav\")\n# For memory purposes, let's cut the recording in half\nn_samples = drums.shape[0]\ncrop = n_samples // 2\n\ndrums = drums[:crop]\nbass = process_audio(stems_dir / \"Bass.wav\")[1][:crop]\nfx = process_audio(stems_dir / \"FX.wav\")[1][:crop]\nvocals = process_audio(stems_dir / \"Vocals.wav\")[1][:crop]\n\nmix_matrix = np.array([0.50, 0.20, 0.15, 0.15])\nmix_func = partial(mix_stems, mix_matrix=mix_matrix)\n\ndrums = mix_func(drums, bass, fx, vocals)\nbass = mix_func(bass, fx, vocals, drums)\nfx = mix_func(fx, vocals, drums, bass)\nvocals = mix_func(vocals, drums, bass, fx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Here is one (blended) stem for reference\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ipd.Audio(fx, rate=sfreq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Separate the components with ICA\nWe'll stack our \"observations\" (the microphone recordings) into a matrix.\nEach column will be a different microphone, and each row will be a different\ntime point. We'll then use ICA to separate the components. By components, we\nmean the original sources that were mixed together to create the microphone\nrecordings (drums, bass, guitars, fx, and vocals).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "microphones = np.vstack([drums, bass, fx, vocals]).T\nica = FastICA(random_state=42)\ncomponents = ica.fit_transform(microphones)\n# Unpack the components\nic_1, ic_2, ic_3, ic_4 = components.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Here are the separated components\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ipd.Audio(normalize_audio(ic_1), rate=sfreq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ipd.Audio(normalize_audio(ic_2), rate=sfreq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ipd.Audio(normalize_audio(ic_3), rate=sfreq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Not too bad!\nThe ICA algorithm was able to separate the components pretty well.\nThe separated components are not exactly the same as the original sources, but\nthey are pretty close. The ICA algorithm is able to separate the components\nbecause the sources are statistically independent. This is a pretty cool\ndemonstration of the power of ICA!\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}