{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Compute Accuracy, Precision, and Recall for a multi-class classification problem\n\nThe MNIST dataset has 10 categories. How do we adopt our classification performance\nmetrics to handle multi-class classification problems?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the MNIST dataset\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "digits = load_digits()\n# Extract the data and labels\nlabels = digits.target\ndata = digits.data\n# Constrain the values of the data to be between 0 and 1\ndata_normalized = data / np.max(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert the data to PyTorch tensors\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_tensor = torch.tensor(data_normalized, dtype=torch.float32)\nlabels_tensor = torch.tensor(labels, dtype=torch.long)\n\n# Split the data into training and testing sets\ntrain_data, test_data, train_labels, test_labels = train_test_split(\n    data_tensor, labels_tensor, test_size=.1, random_state=42\n)\n\n# Convert the data to PyTorch Datasets\ntrain_data_ds = torch.utils.data.TensorDataset(train_data, train_labels)\ntest_data_ds = torch.utils.data.TensorDataset(test_data, test_labels)\n\n# Convert the data to PyTorch DataLoaders objects\nbatch_size = 32\ntrain_loader = DataLoader(train_data_ds, batch_size=batch_size, shuffle=True, drop_last=True)\ntest_loader = DataLoader(test_data_ds, batch_size=batch_size, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the neural network\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DigitsNet(nn.Module):\n    def __init__(self):\n        super(DigitsNet, self).__init__()\n\n        # Define the layers\n        self.input = nn.Linear(64, 32)\n        self.fc1 = nn.Linear(32, 16)\n        self.fc2 = nn.Linear(16, 16)\n        self.output = nn.Linear(16, 10)\n\n    def forward(self, x):\n        x = F.relu(self.input(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.output(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "digits_net = DigitsNet()\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(digits_net.parameters(), lr=.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the neural network\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_epochs = 10\nlosses = torch.zeros(n_epochs)\ntrain_accuracies = []\ntest_accuracies = []\n\nfor ei in range(n_epochs):\n    digits_net.train() # set the model to training mode\n\n    batch_accuracies = []\n    batch_losses = []\n    for data, labels in train_loader:\n        output = digits_net(data)\n        loss = loss_fn(output, labels)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # This batches loss\n        batch_losses.append(loss.item())\n        # Compute this batches accuracy\n        matches = torch.argmax(output, dim=1) == labels\n        matches_float = matches.float() # go from True/False to 1/0\n        accuracy_percentage = 100 * matches_float.mean()\n        batch_accuracies.append(accuracy_percentage)\n\n    # Compute the average loss and accuracy across batches for this epoch\n    losses[ei] = np.mean(batch_losses)\n    train_accuracies.append(np.mean(batch_accuracies))\n\n    # Compute the accuracy on the test set\n    test_data, test_labels = next(iter(test_loader))\n    with torch.no_grad():\n        test_output = digits_net(test_data)\n    matches = torch.argmax(test_output, dim=1) == test_labels\n    matches_float = matches.float()\n    test_accuracy_percentage = 100 * matches_float.mean()\n    test_accuracies.append(test_accuracy_percentage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the results\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 3), constrained_layout=True)\n\nax[0].plot(losses, label='Training Loss')\nax[0].set_xlabel('Epoch')\nax[0].set_ylabel('Loss')\nax[0].legend()\n\nax[1].plot(train_accuracies, label='Training Accuracy')\nax[1].plot(test_accuracies, label='Test Accuracy')\nax[1].set_xlabel('Epoch')\nax[1].set_ylabel('Accuracy (%)')\nax[1].legend()\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Accuracy, Precision, and Recall\n\nUnlike the previous example (on the wine dataset), the MNIST dataset has 10 categories,\nso there are 10 accuracy/precision/recall values to compute.\nWe have 3 options:\n- Compute the metrics for each class separately\n- Compute the metrics for each class and average them (unweighted average)\n- Compute the metrics for each class and average them (weighted average)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_hat = digits_net(train_loader.dataset.tensors[0])\ntrain_predictions = torch.argmax(y_hat, dim=1)\n\ny_hat = digits_net(test_loader.dataset.tensors[0])\ntest_predictions = torch.argmax(y_hat, dim=1)\n\nprecision = precision_score(\n    test_loader.dataset.tensors[1].detach().numpy(),\n    test_predictions.detach().numpy(),\n    average=None\n)\nrecall = recall_score(\n    test_loader.dataset.tensors[1],\n    test_predictions,\n    average=None\n)\n\nfig, ax = plt.subplots(figsize=(10, 4), constrained_layout=True)\n\nax.bar(\n    np.arange(10) - .2, precision, width=.4, label='Precision'\n)\nax.bar(\n    np.arange(10) + .2, recall, width=.4, label='Recall'\n)\nax.set_xticks(np.arange(10))\nax.set_xlabel('Category')\nax.set_title('Precision and Recall for each category')\nax.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the average precision and recall\n\nFor the training and testing sets\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n\ntrain_metrics = [0, 0, 0 ,0]\ntest_metrics = [0, 0, 0, 0]\n\ntrain_metrics[0] = accuracy_score(train_loader.dataset.tensors[1], train_predictions)\ntrain_metrics[1] = precision_score(train_loader.dataset.tensors[1], train_predictions, average='macro')\ntrain_metrics[2] = recall_score(train_loader.dataset.tensors[1], train_predictions, average='macro')\ntrain_metrics[3] = f1_score(train_loader.dataset.tensors[1], train_predictions, average='macro')\n\ntest_metrics[0] = accuracy_score(test_loader.dataset.tensors[1], test_predictions)\ntest_metrics[1] = precision_score(test_loader.dataset.tensors[1], test_predictions, average='macro')\ntest_metrics[2] = recall_score(test_loader.dataset.tensors[1], test_predictions, average='macro')\ntest_metrics[3] = f1_score(test_loader.dataset.tensors[1], test_predictions, average='macro')\n\nax.bar(\n    np.arange(4) - .2, train_metrics, width=.4, label='Training Set'\n)\nax.bar(\n    np.arange(4) + .2, test_metrics, width=.4, label='Test Set'\n)\nax.set_xticks(np.arange(4))\nax.set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1'])\nax.set_title('Metrics for the training and testing sets')\nax.legend()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}